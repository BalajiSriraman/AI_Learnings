{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: nltk in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: click in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: wrapt in /Users/balaji/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"I love this product\", \"Positive\"),\n",
    "    (\"This is the best experience ever\", \"Positive\"),\n",
    "    (\"I feel amazing after using this\", \"Positive\"),\n",
    "    (\"I enjoy this app\", \"Positive\"),\n",
    "    (\"This product is excellent\", \"Positive\"),\n",
    "    (\"Absolutely wonderful service\", \"Positive\"),\n",
    "    (\"Great quality and performance\", \"Positive\"),\n",
    "    (\"I am very satisfied\", \"Positive\"),\n",
    "    (\"Highly recommend this product\", \"Positive\"),\n",
    "    (\"It works perfectly\", \"Positive\"),\n",
    "    (\"I hate this product\", \"Negative\"),\n",
    "    (\"This is the worst experience\", \"Negative\"),\n",
    "    (\"I feel terrible using this\", \"Negative\"),\n",
    "    (\"Very disappointed with this purchase\", \"Negative\"),\n",
    "    (\"Worst service I have ever experienced\", \"Negative\"),\n",
    "    (\"I will never buy this again\", \"Negative\"),\n",
    "    (\"Horrible quality and performance\", \"Negative\"),\n",
    "    (\"This app is so frustrating\", \"Negative\"),\n",
    "    (\"I regret buying this\", \"Negative\"),\n",
    "    (\"The product stopped working in a week\", \"Negative\"),\n",
    "    (\"Fantastic customer support!\", \"Positive\"),\n",
    "    (\"This exceeded my expectations\", \"Positive\"),\n",
    "    (\"I use this daily and love it\", \"Positive\"),\n",
    "    (\"Very efficient and reliable\", \"Positive\"),\n",
    "    (\"I had a pleasant experience\", \"Positive\"),\n",
    "    (\"Not worth the money\", \"Negative\"),\n",
    "    (\"The delivery was slow and product was damaged\", \"Negative\"),\n",
    "    (\"It broke after a few uses\", \"Negative\"),\n",
    "    (\"Waste of money\", \"Negative\"),\n",
    "    (\"I had high hopes but it disappointed me\", \"Negative\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "sentences , labels = zip(*data)\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Word2Vec model\n",
    "# word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "  # convert it into tokens\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  text = text.lower()\n",
    "  tokens = word_tokenize(text)\n",
    "  # remove stop words\n",
    "  filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "  return ' '.join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Function to compute sentence embeddings by averaging word vectors\n",
    "# normalising labelled data.\n",
    "def get_sentence_embedding(sentense_tokens, word2vec_model):\n",
    "  word_vectors = [word2vec_model.wv[word] for word in sentense_tokens if word in word2vec_model.wv]\n",
    "  if(len(word_vectors)> 0):\n",
    "    # Axis 0, 1st Column\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "  \n",
    "  # Return a zero vector if no words are in the model\n",
    "  return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "\n",
    "X = np.array([get_sentence_embedding(sentence, word2vec_model) for sentence in tokenized_sentences])\n",
    "y = np.array(labels)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# stratify=y ensures that the class distribution is maintained across the train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# classifier = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "# C is the inverse of regularization strength; smaller values specify stronger regularization.\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression(C=1))\n",
    "# Train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love this pen -> Sentiment: Positive\n",
      "Sentence: This is the worst experience -> Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\"I love this pen\", \"This is the worst experience\"]\n",
    "new_tokenized = [word_tokenize(sentence.lower()) for sentence in new_sentences]\n",
    "new_embeddings = np.array([get_sentence_embedding(tokens, word2vec_model) for tokens in new_tokenized])\n",
    "\n",
    "predictions = classifier.predict(new_embeddings)\n",
    "for sentence, prediction in zip(new_sentences, predictions):\n",
    "    print(f\"Sentence: {sentence} -> Sentiment: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
