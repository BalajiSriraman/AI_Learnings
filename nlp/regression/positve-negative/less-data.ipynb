{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Collecting scipy<1.14.0,>=1.7.0\n",
      "  Using cached scipy-1.13.1-cp310-cp310-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Collecting numpy<2.0,>=1.18.5\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, tqdm, threadpoolctl, regex, numpy, joblib, click, smart-open, scipy, nltk, scikit-learn, gensim\n",
      "Successfully installed click-8.1.8 gensim-4.3.3 joblib-1.4.2 nltk-3.9.1 numpy-1.26.4 regex-2024.11.6 scikit-learn-1.6.1 scipy-1.13.1 smart-open-7.1.0 threadpoolctl-3.6.0 tqdm-4.67.1 wrapt-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/balaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"I love this product\", \"Positive\"),\n",
    "    (\"This is the best experience ever\", \"Positive\"),\n",
    "    (\"I feel amazing after using this\", \"Positive\"),\n",
    "    (\"I hate this product\", \"Negative\"),\n",
    "    (\"This is the worst experience\", \"Negative\"),\n",
    "    (\"I feel terrible using this\", \"Negative\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences , labels = zip(*data)\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Function to compute sentence embeddings by averaging word vectors\n",
    "# normalising labelled data.\n",
    "def get_sentence_embedding(sentense_tokens, word2vec_model):\n",
    "  word_vectors = [word2vec_model.wv[word] for word in sentense_tokens if word in word2vec_model.wv]\n",
    "  if(len(word_vectors)> 0):\n",
    "    # Axis 0, 1st Column\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "  \n",
    "  # Return a zero vector if no words are in the model\n",
    "  return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "\n",
    "X = np.array([get_sentence_embedding(sentence, word2vec_model) for sentence in tokenized_sentences])\n",
    "y = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "classifier = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "# Train\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Test\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I love this pen -> Sentiment: Negative\n",
      "Sentence: This is the worst experience -> Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\"I love this pen\", \"This is the worst experience\"]\n",
    "new_tokenized = [word_tokenize(sentence.lower()) for sentence in new_sentences]\n",
    "new_embeddings = np.array([get_sentence_embedding(tokens, word2vec_model) for tokens in new_tokenized])\n",
    "\n",
    "predictions = classifier.predict(new_embeddings)\n",
    "for sentence, prediction in zip(new_sentences, predictions):\n",
    "    print(f\"Sentence: {sentence} -> Sentiment: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
